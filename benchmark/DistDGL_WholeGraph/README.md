This folder is extracted from nvcr.io/nvidia/dgl:23.11-py3 /workspace/examples/wholegraph-examples/DistDGL-WholeGraph

## Distributed GNN Training with DistDGL and WholeGraph (DistDGL + WholeGraph)

This example demonstrates the utilization of DistDGL for large-scale distributed (multi-node multi-GPU) GNN training, where graph features are managed by WholeMemory, and graph strucutre is managed by DistDGL ([distributed DGL](https://docs.dgl.ai/en/latest/api/python/dgl.distributed.html)). This example is built upon an official [DistDGL example](https://github.com/dmlc/dgl/blob/master/examples/distributed/graphsage/node_classification.py), adding WholeGraph support. Within the example, a GraphSAGE model is used for a node prediction task with ogbn-papers100M opensource dataset.

In this example, graph structure is partitioned via a `partition_graph.py` script and sampled using DistDGL, while graph features are distributed across all ranks via WholeGraph's memory module with its [`distributed`](https://github.com/rapidsai/wholegraph/blob/branch-23.10/docs/wholegraph/source/basics/wholegraph_intro.md) type.

## How to use this example?

### Step 1: Prepare the data files for DGL and WholeGraph
Launch a DGL container with interactive mode.
```bash
docker run --gpus all -it --rm -v /scratch:/data  nvcr.io/nvidia/dgl:23.11-py3
```
Here, we mount the `scratch` directory into the container, which should be part of the cluster's shared filesystem, accessible to all compute nodes within the cluster.

Within the container, under this example directory, locate and run the `partition_graph.py` script to preprocess and partition ogbn-papers100M dataset. The downloaded graph dataset should be presented at `--root-dir` and the partitioned graph strucutre and graph features binary file are saved to the location specified by argument `--output`.

```bash
cd /workspace/nvdia-examples/wholegraph-examples/DistDGL-WholeGraph
python partition_graph.py --dataset ogbn-papers100M --root-dir /data --output /data --num_parts 2 --balance_train --undirected --balance_edges --num_trainers_per_machine 4 --use-wm --keep-dgl-features
```

Note that you don't need to repeat Step 1 for every training session, as long as the two files are already in place.

### Step 2: Setup IP configuration file

User needs to set their own IP configuration file `ip_config.txt` before training. For example, in a Slurm environment, the IP configuration file can be generated by:

```bash
scontrol show hostname | xargs -n1 host | awk '/has address/ {print $4}' > ./ip_config.txt
```
And if we have two machines available, the generated `ip_config.txt` could look like:

```
172.31.19.1
172.31.23.205
```
Note that the ip addresses shown in your ip_config.txt file might be different from this example.

### Step 3: Launch the training

Within the same directory, locate and launch the training using script `launch.py`, where it issues multiple job steps of interactive `srun` to launch DistDGL (RPC server/client processes) on a multi-node cluster. Note that you need to have [Pyxis](https://github.com/NVIDIA/pyxis) installed with Slurm manager to easily run containered workloads in the cluster. For example, the following commands will launch the node classification task on a two node cluster with a sbatch command.

```bash
#!/bin/bash
## Slurm job setup
#SBATCH -t 00:30:00     #wall time limit, hr:min:sec
#SBATCH -N 2            #number of nodes
#SBATCH -p ...

CONT='nvcr.io/nvidia/dgl:23.11-py3'
CONT_NAME=cont
# launch/initialize the container
srun -ntasks-per-node=1 --container-image=$CONT --container-name=$CONT_NAME true
# specify # of trainers(gpus), # of samplers and # of servers per machine.
NUM_TRAINERS=4
NUM_SERVERS=1
NUM_SAMPLERS=0
# both WORKSPACE and RUNDIR will be mounted/visible inside the container
WORKSPACE=/data
RUNDIR=/workspace/nvidia-examples/wholegraph-examples/DistDGL-WholeGraph
scontrol show hostname | xargs -n1 host | awk '/has address/ {print $4}' > $RUNDIR/ip_config.txt
# launch all training(client) processes once through srun (slurm); or launch it one node at a time through torchrun  (pytorch)
DGL_CLIENT_LAUNCH=srun # needs [srun|pytorch] and MUST be consistent to the training script argument `--wg-launch-agent` [mpi|pytorch], respectively.

PART_CONFIG=$WORKSPACE/data/ogbn-papers100M.json
IP_CONFIG=$RUNDIR/ip_config.txt

python3 ${RUNDIR}/launch.py --container_name ${CONT_NAME} --workspace ${WORKSPACE} --num_trainers ${NUM_TRAINERS} --num_samplers ${NUM_SAMPLERS} \
 --rundir ${RUNDIR} \
 --part_config ${PART_CONFIG} \
 --num_servers ${NUM_SERVERS} \
 --ip_config ${IP_CONFIG} \
 --client-launcher ${DGL_CLIENT_LAUNCH} \
 "python3 node_classification.py --graph_name ogbn-papers100M --ip_config ip_config.txt --num_epochs 1 --batch_size 1000 --ngpu-per-node ${NUM_TRAINERS} --num_hidden 256 --use-wm --wg-launch-agent mpi --wg-comm-backend nvshmem"
```

In the above command, the number of rows in `ip_config.txt` determines how many nodes in the cluster you will run. The 4 after `NUM_TRAINERS` specifies how many training processes will be launched on each node. Usually each training process attaches to 1 GPU. The 1 after `NUM_SERVERS` specifies how many server process will be launched on each node, and `NUM_SAMPLERS` indicates the number of CPU sampling process will be used for each node. Note that only CPU sampling is supported within DistDGL. And the flag `--use-wm` tells that WholeMemory (wm) will be enabled to manage graph features. Assuming the Slurm manager comes with MPI support,  here, `--wg-launch-agent` is set to `mpi`, since the training processes (client processes) are launched by Slurm (srun) by `--client-launcher srun`. The argument `--wg-comm-backend` specifies what communication library (either `nccl` or `nvshmem`) to use for gathering features. Note that when NVSHMEM is selected as the WholeGraph communication library (`--wg-comm-backend nvshmem`), the job launcher (`--client-launcher` and `--wg-launch-agent`) can not be the `pytorch` distributed module (or `torchrun`), but only the Slurm (use `srun`) or MPI (use `mpirun`).
